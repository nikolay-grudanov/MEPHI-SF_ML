# Обучение с подкреплением - МИФИ 3 семестр

## Лекция 1 (02.09.2025)
**Тема:** Введение в обучение с подкреплением (RL): Актуальность, примеры и основные компоненты.

**Ключевые концепции:**
- Обучение с подкреплением (RL) как актуальная область искусственного интеллекта.
- Ключевые компоненты RL-задачи: Агент, Среда (Environment), Состояние/Наблюдение, Действие, Награда/Штраф (Reward).
- Примеры применения RL: робототехника (склад), онлайн-реклама, системы рекомендаций, диалоговые системы (чат-боты).

**Основные формулы/алгоритмы:**
- Общее представление о MDP (Markov Decision Processes).
- Дилемма исследования vs. эксплуатации (будущее рассмотрение).

**Связь с другими лекциями:**
- Курс будет подробно изучать MDP, динамическое программирование и Model-Free RL.
- Предварительный семинар по основам PyTorch для работы с алгоритмами.
- Введение в формализацию задачи (MDP) (следующая лекция).

---

## Лекция 2 (04.09.2025)
**Тема:** Марковские процессы принятия решений (MDP), дисконтирование и функции ценности.

**Ключевые концепции:**
- Формализация RL-задачи как MDP (кортеж: S, A, P, R, $\gamma$).
- Свойство Марковости (независимость будущего от прошлого).
- Кумулятивная награда (Return, $G_t$) и горизонт планирования.
- Дисконтирование ($\gamma$) и его влияние на горизонт планирования.
- Функции ценности состояния ($V^\pi(S)$) и действия ($Q^\pi(S, A)$).

**Основные формулы/алгоритмы:**
- Рекурсивная формула для кумулятивной награды: $G_t = R_{t+1} + \gamma G_{t+1}$.
- Первые два уравнения Беллмана для оценки ценности заданной политики ($V^\pi, Q^\pi$).
- Проблема Reward Hacking (неправильный дизайн награды).

**Связь с другими лекциями:**
- Поиск оптимальной политики (следующая лекция).
- Динамическое программирование как метод решения задач, когда MDP известно.

---

## Лекция 3 (06.09.2025)
**Тема:** Семинар: Основы PyTorch и работа с тензорами.

**Ключевые концепции:**
- Тензоры PyTorch и их отличие от NumPy (requires\_grad, общая память).
- Автоматическое вычисление градиентов (AutoGrad, метод `.backward()`).
- Структура нейросетевых слоев (NN.Linear) и их работа с батчами.
- Функции активации для добавления нелинейности (Sigmoid, ReLU, SoftMax).
- Процесс обучения нейросети (прямой проход, подсчет потерь, обратное распространение, шаг оптимизатора).

**Основные формулы/алгоритмы:**
- Оптимизаторы (например, Adam) и Learning Rate.
- Градиентный спуск.

**Связь с другими лекциями:**
- Навыки PyTorch необходимы, начиная с третьей недели курса.

---

## Лекция 4 (09.09.2025)
**Тема:** Оптимальные политики и уравнения Беллмана для оптимальности.

**Ключевые концепции:**
- Политика как распределение вероятности выбора действий в состоянии S.
- Оптимальная политика ($\pi^*$).
- Эквивалентность сравнения политик и функций ценности.
- Полезность действия (Q-функция) как основа для выбора жадного действия.

**Основные формулы/алгоритмы:**
- Уравнения Беллмана для оптимальности (использование максимизации по действиям).
- Расчет $G_t$ для конкретной траектории.

**Связь с другими лекциями:**
- Динамическое программирование (Value Iteration и Policy Iteration) как метод решения MDP с полной информацией (следующая лекция).

---

## Лекция 5 (11.09.2025)
**Тема:** Динамическое программирование: Value Iteration (VI) и Policy Iteration (PI).

**Ключевые концепции:**
- Динамическое программирование (DP) как метод поиска оптимальной политики, требующий **полного знания MDP**.
- Итеративное обновление ценности (Value Iteration).
- Теорема о сжимающем операторе (гарантия сходимости DP).
- Policy Iteration: чередование **Оценки политики** ($V^\pi$) и **Жадного улучшения политики** ($\pi'$).
- Проблема высокой вычислительной сложности DP (зависимость $O(S^2 \cdot A)$).

**Основные формулы/алгоритмы:**
- Итеративное правило обновления ценности ($V^{k+1}$) в Value Iteration.
- Улучшение политики (Policy Improvement) через $\arg\max$ по $Q$-функции.

**Связь с другими лекциями:**
- Примеры: Grid World (распространение волны ценностей).
- Необходимость перехода к Model-Free методам из-за ограничений DP.

---

## Лекция 6 (13.09.2025)
**Тема:** Семинар: Gymnasium, дискретизация состояний и Model-Free RL.

**Ключевые концепции:**
- Фреймворк Gymnasium (Gym) и его методы (`reset`, `step`, `render`).
- Флаги завершения эпизода (`terminated` и `truncated`).
- Среда Mountain Car (движение каретки на горке).
- Непрерывность состояния (позиция, скорость) как проблема для табличных методов.
- **Дискретизация состояния** (State Discretization) для применения Value Iteration.

**Основные формулы/алгоритмы:**
- Эмпирическая стратегия решения Mountain Car (раскачивание).
- Дискретизация непрерывных координат в индексы.
- Value Iteration на дискретизированном пространстве.

**Связь с другими лекциями:**
- Необходимость Model-Free методов, так как MDP часто неизвестно (L7).

---

## Лекция 7 (16.09.2025)
**Тема:** Model-Free RL (I): Введение, Model-Based vs Model-Free, методы Монте-Карло.

**Ключевые концепции:**
- **Model-Based RL**: Требует знания или обучения модели среды (функции переходов $P$ и награды $R$).
- **Model-Free RL**: Обучение функций ценности или политики напрямую на опыте.
- Минусы Model-Based (неточность аппроксимации, много данных).
- Плюсы Model-Based (Sample Efficiency, планирование).
- Методы Монте-Карло (MC) для оценки Q-функции.
- **First Visit MC** vs **Every Visit MC**.

**Основные формулы/алгоритмы:**
- Плавное обновление Q-функции с коэффициентом $\alpha$.
- $\epsilon$-greedy стратегия для баланса исследования и эксплуатации.
- Softmax с температурой и UCB (Upper Confidence Bound) для исследования.

**Связь с другими лекциями:**
- Необходимость обновления Q-функции без полных эпизодов (переход к TD).

---

## Лекция 8 (18.09.2025)
**Тема:** Model-Free RL (II): TD Learning, SARSA, Q-Learning, On-Policy vs Off-Policy.

**Ключевые концепции:**
- **Temporal Difference (TD) Learning**: Обновление ценности через оценку ценности (бутстраппинг).
- Преимущества TD перед MC (не нужно ждать конца эпизода, меньшая дисперсия).
- **On-Policy RL**: Обучение на данных, собранных текущей политикой (например, SARSA).
- **Off-Policy RL**: Обучение на данных, собранных другой политикой (например, Q-Learning).

**Основные формулы/алгоритмы:**
- TD-Target: $R_{t+1} + \gamma V(S_{t+1})$.
- SARSA Update (S, A, R, S', A').
- Q-Learning Update (использует $\max_{A'} Q(S', A')$ для оценки).

**Связь с другими лекциями:**
- Пример Cliff Walking: SARSA (безопасный путь) vs Q-Learning (оптимальный, но рискованный).
- Подход Expected SARSA (снижение дисперсии).

---

## Лекция 9 (20.09.2025)
**Тема:** Семинар: Реализация SARSA, Q-Learning и Expected SARSA.

**Ключевые концепции:**
- Практическая реализация Q-Learning агента.
- Инициализация и обновление Q-таблицы.
- Реализация $\epsilon$-greedy политики.
- Сравнение On-Policy (SARSA) и Off-Policy (Q-Learning, Expected SARSA) в среде Cliff Walking.
- Expected SARSA: использование полного мат. ожидания по всем возможным $A'$ для оценки.

**Основные формулы/алгоритмы:**
- Q-Learning Update: $Q(S, A) \leftarrow Q(S, A) + \alpha \cdot (R + \gamma \max_{A'} Q(S', A') - Q(S, A))$.
- Expected SARSA Update (использование вероятностей $\epsilon$-greedy политики для оценки $Q(S', \cdot)$).

**Связь с другими лекциями:**
- Демонстрация проблемы Q-Learning: нацеленность на оптимальную, но рискованную стратегию, игнорируя риски исследования.

---

## Лекция 10 (23.09.2025)
**Тема:** Deep Q-Networks (DQN): Нейросети как аппроксиматоры Q-функции.

**Ключевые концепции:**
- Проблема масштабирования табличных методов на среды с **непрерывным пространством состояний** (изображения, физика).
- Использование нейросетей для аппроксимации Q-функции.
- Архитектура DQN (Conv/Dense слои) для обработки изображений (Atari).
- TD-Ошибка (TD-Error) как функция потерь.
- Связь с задачей регрессии (подгонка текущей Q к более точной целевой Q).

**Основные формулы/алгоритмы:**
- Лосс DQN (MSE/L2 между текущей $Q$ и целевой $Y_t$).

**Связь с другими лекциями:**
- Проблемы нестабильности обучения DQN и методы их решения (следующая лекция).
- Целевое значение $Y_t$ как изменяющаяся цель (Target Network).

---

## Лекция 11 (25.09.2025)
**Тема:** Улучшения DQN: Target Network, Experience Replay, Double DQN.

**Ключевые концепции:**
- Проблема **коррелированных данных** в Deep RL.
- **Experience Replay (Replay Buffer)**: хранение и случайное сэмплирование переходов для декорреляции данных.
- Проблема **убегающей цели** (Target instability).
- **Target Network**: заморозка весов целевой сети $Q_{\text{target}}$ на $C$ шагов для стабильности.
- **Double DQN (DDQN)**: разделение выбора действия и оценки ценности для борьбы с переоценкой Q-функции.
- **Dueling Networks** (разделение на V-value и Advantage).
- Prioritized Experience Replay (PER) (сэмплирование по величине TD-ошибки).

**Основные формулы/алгоритмы:**
- Обновление весов Target Network ($\theta_{\text{target}}$).
- Prioritized Experience Replay (вероятность, пропорциональная TD-Error).

**Связь с другими лекциями:**
- DQN как Model-Free Off-Policy алгоритм, применимый с Replay Buffer.
- Практическое применение DQN на примере торговли акциями.

---

## Лекция 12 (27.09.2025)
**Тема:** Эволюционные стратегии (ES) и Cross-Entropy Method (CEM).

**Ключевые концепции:**
- **Evolution Strategies (ES)**: Методы, вдохновленные биологической эволюцией.
- **Преимущества ES**: Не требует обратного распространения ошибки (BackProp), легко масштабируется, эффективен при недифференцируемых наградах.
- Популяция кандидатов (политик/траекторий).
- **Cross-Entropy Method (CEM)**: Отбор N наилучших траекторий (элитные сессии) и обучение политики на них как задача классификации.
- Alpha Evolve (современное применение ES с LLM для генерации кода).

**Основные формулы/алгоритмы:**
- Алгоритм CEM (сбор сессий, расчет перцентиля, отбор элитных действий).
- Обновление политики (плавный переход к новой политике через Learning Rate).

**Связь с другими лекциями:**
- Сходство с методом Монте-Карло (сбор полных траекторий).
- CEM как альтернатива Policy Gradient методам.

---

## Лекция 13 (30.09.2025)
**Тема:** Policy Gradient (I): Теорема REINFORCE и обучение политики.

**Ключевые концепции:**
- Прямое обучение политики: целевая функция — ожидаемая кумулятивная награда.
- **Сети политик** (Policy Networks) для дискретных и непрерывных действий.
- Проблема недифференцируемости сэмплирования.
- **Репараметризационный трюк** (Reparametrization Trick) для обхода недифференцируемости.

**Основные формулы/алгоритмы:**
- Вероятность траектории.
- **Теорема Policy Gradient (REINFORCE)**: $\nabla_\theta J(\theta) = E_{\tau \sim \pi_\theta} [\sum_{t=0}^T \nabla_\theta \log \pi_\theta (A_t|S_t) \cdot G_t]$.
- Использование $\log$ Derivative Trick.

**Связь с другими лекциями:**
- REINFORCE требует полных траекторий (проблема высокой дисперсии).
- Actor-Critic и Advantage (L14) как метод снижения дисперсии.

---

## Лекция 14 (02.10.2025)
**Тема:** Policy Gradient (II): Actor-Critic, Advantage и GAE.

**Ключевые концепции:**
- **Actor-Critic (AC) архитектура**: Актер (Policy Network) обучает политику, Критик (Value Network) оценивает ценность состояния/действия.
- Actor обновляется по Policy Gradient, Critic — по TD-ошибке.
- **Базовая функция (Baseline)**: вычитание $V(S)$ для снижения дисперсии градиента.
- **Advantage Function ($A(S, A)$)**: $Q(S, A) - V(S)$.
- Интерпретация Advantage: насколько действие лучше, чем "среднее" ожидаемое действие в данном состоянии.
- Использование TD-оценки для обучения Critic.

**Основные формулы/алгоритмы:**
- Обновление Actor с использованием Advantage: $J(\theta) \leftarrow \nabla_\theta \log \pi_\theta (A_t|S_t) \cdot A_t$.
- Оценка Advantage через $V$-функцию (TD-ошибка): $R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$.
- GAE (Generalized Advantage Estimation) для более точной оценки Advantage.

**Связь с другими лекциями:**
- Policy Gradient Actor-Critic как более стабильный алгоритм по сравнению с REINFORCE.

---

## Лекция 15 (04.10.2025)
**Тема:** Продвинутые Policy Gradient: TRPO, PPO, SAC.

**Ключевые концепции:**
- Проблема нестабильности и **большого шага градиента** (пример с казино).
- Необходимость ограничения изменения политики.
- **Importance Sampling (IS)**: оценка градиента новой политики с помощью данных старой политики.
- **TRPO (Trust Region Policy Optimization)**: ограничение обновления политики через KL-дивергенцию.
- Сложность TRPO: оптимизация второго порядка и матрица Фишера.
- **PPO (Proximal Policy Optimization)**: упрощенный и стабильный алгоритм.
- PPO Clip Objective (клипирование отношения вероятностей).
- **Soft Actor-Critic (SAC)**: максимизация энтропии для более широкого исследования.

**Основные формулы/алгоритмы:**
- Отношение вероятностей ($R$).
- PPO Clip Objective (минимизация пессимистичной оценки).
- Лосс SAC (максимизация награды + энтропии).

**Связь с другими лекциями:**
- PPO как стандартный алгоритм для практического применения.
- Различие в подходе к исследованию: PPO (стахастичность политики) vs SAC (встроенная максимизация энтропии).

---

## Лекция 16 (07.10.2025)
**Тема:** Offline RL и Imitation Learning.

**Ключевые концепции:**
- **Мотивация Offline RL**: Дороговизна онлайн-взаимодействия, желание использовать большие, фиксированные датасеты.
- **Imitation Learning (IL)**: обучение на экспертных данных без функции награды.
- **Behavioral Cloning (BC)**: прямое копирование поведения эксперта (Supervised Learning).
- Проблема BC: ошибки в незнакомых состояниях.
- **Dagger (Dataset Aggregation)**: инкрементальное улучшение BC с интерактивным участием эксперта.
- **Offline RL**: поиск оптимальной политики на фиксированном, разнообразном датасете.

**Основные формулы/алгоритмы:**
- Лосс BC: минимизация ошибки (MSE или Cross-Entropy) между политикой агента и эксперта.
- Dagger Algorithm (сбор новых данных в процессе работы).

**Связь с другими лекциями:**
- Проблема переоценки Q-функции (Overestimation) в Offline RL (L17).
- Offline RL алгоритмы (BCQ, CQL).

---

## Лекция 17 (09.10.2025)
**Тема:** Inverse RL (IRL): Восстановление функции награды.

**Ключевые концепции:**
- **Постановка задачи IRL**: Имея оптимальную политику и динамику среды, восстановить функцию награды.
- Мотивация IRL: Копирование намерений эксперта, а не только действий.
- Проблема **неоднозначности** функции награды (множество функций могут объяснить одну политику).
- **Линейная функция награды**: $R(S, A) = \mathbf{w}^T \phi(S, A)$.
- **Суммарный признак (Feature Expectation, $\mu$)**: мат. ожидание дисконтированных признаков траектории.

**Основные формулы/алгоритмы:**
- Maximum Margin IRL (максимизация отступа между экспертным и лучшим неэкспертным действием).
- **Maximum Entropy IRL**: поиск функции награды, максимизирующей энтропию распределения траекторий.
- Алгоритм Max Entropy IRL (итеративное обучение награды и оптимальной политики).

**Связь с другими лекциями:**
- IRL как метод, позволяющий получить награду там, где ее сложно сформулировать (для дальнейшего использования в RL).

---

## Лекция 18 (11.10.2025)
**Тема:** Семинар: Планирование, алгоритмы A* и Monte Carlo Tree Search (MCTS).

**Ключевые концепции:**
- **Планирование**: процесс построения последовательности действий, требующий **модели среды**.
- Алгоритмы поиска пути: Dijkstra, A*.
- **MCTS (Monte Carlo Tree Search)**: алгоритм для планирования в средах с большим пространством состояний (например, игры).
- Четыре стадии MCTS: Selection (выбор узла), Expansion (расширение), Simulation/Rollout (проигрыш эпизода), Backpropagation (обратное распространение ценности).
- **Adversarial Environment** (учет противника).

**Основные формулы/алгоритмы:**
- Эвристическая функция F в A* (расстояние до старта + расстояние до цели).
- **UCB (Upper Confidence Bound)**: формула для выбора узла в MCTS (баланс ценности Q и исследования).

**Связь с другими лекциями:**
- Value Iteration vs. Dijkstra (сравнение DP и поиска пути).

---

## Лекция 19 (14.10.2025)
**Тема:** Мультиагентное обучение (MARL) и RL для языковых моделей (LLMs).

**Ключевые концепции:**
- **MARL**: Нестационарность среды (агенты меняют среду вокруг себя).
- Проблемы MARL: **Credit Assignment** (кто внес вклад в общую награду), Масштабируемость.
- **CTDE (Centralized Training Decentralized Execution)**: Централизованный критик для обучения, децентрализованный актер для исполнения.
- Примеры: VDN/QMIX (факторизация Q-функции), MARL для управления трафиком.
- **RLHF (RL from Human Feedback)**: интеграция человеческих предпочтений в LLMs.
- **Reward Model**: нейросеть, обученная на ранжировании ответов.
- DPO (Direct Policy Optimization) и GRPO (методы без критика для стабильности).

**Основные формулы/алгоритмы:**
- Пространство действий в MARL: экспоненциальный рост $A = \prod A_i$.

**Связь с другими лекциями:**
- Применение PPO в RLHF.

---

## Лекция 20 (16.10.2025)
**Тема:** VLA (Vision Language Action) Модели и Автономное вождение.

**Ключевые концепции:**
- **VLA Модели**: Объединение зрения (Vision), языка (Language) и действий (Action) для обобщенного управления роботами.
- **Foundation Models** в робототехнике: Генерализация по разным роботам (Embodiment) и задачам (Task).
- **Автономное вождение**: Модульный подход (Perception, Planning, Control).
- Метрики для оценки траекторий: **Безопасность** (TTC), **Комфорт** (Jerk), **Точность** (FDE/ADE).
- Сценарии оценки: **Open Loop** (сравнение с экспертом) vs **Close Loop** (полная симуляция).
- Подходы: Imitation Learning, Offline RL, Sim2Real.

**Основные формулы/алгоритмы:**
- Метрики jerk и yaw rate для оценки комфорта.

**Связь с другими лекциями:**
- Sim2Real (проблема переноса из симулятора в реальный мир).
- Контрастивное обучение в Imitation Learning для добавления негативных сценариев.

---

## Лекция 21 (18.10.2025)
**Тема:** Исследование (Exploration) и Обучение на данных без действий.

**Ключевые концепции:**
- **Внутренняя мотивация (Intrinsic Motivation)**: дополнительная награда для поощрения исследования среды.
- **Random Network Distillation (RND)**: использование случайной сети для оценки новизны состояния (Curiosity).
- **Проблема шумного телевизора** (Noise TV).
- **Inverse Curiosity Model (ICM)**: обучение модели инверсной динамики для фильтрации неконтролируемого шума.
- **Обучение без действий (Learning from Observations)**: использование неразмеченных видеоданных (YouTube).
- Модель **Инверсной динамики (IDM)**: предсказание действия по $S_t$ и $S_{t+1}$ для разметки неразмеченных данных.

**Основные формулы/алгоритмы:**
- RND Intrinsic Reward: $R_{\text{int}} = \text{MSE}(\text{Predictor}(S), \text{Target}(S))$.

**Связь с другими лекциями:**
- Применение IDM для улучшения Behavioral Cloning (Minecraft).
- Общие итоги курса и основные направления.

---