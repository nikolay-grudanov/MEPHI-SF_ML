{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание Этика искусственного интеллекта\n",
    "\n",
    "Выполнил: Груданов Николай Алексеевич"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка среды\n",
    "\n",
    "Подготовим среду для выполнения задания. Загрузим все необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pandas version: 2.2.2\n",
      " numpy version: 1.26.4\n",
      " sklearn version: 1.5.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "print(f' pandas version: {pd.__version__}')\n",
    "print(f' numpy version: {np.__version__}')\n",
    "print(f' sklearn version: {sklearn.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим датасет\n",
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "target",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "comment_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "severe_toxicity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "obscene",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "identity_attack",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "insult",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "threat",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "asian",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "atheist",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "bisexual",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "black",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "buddhist",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "christian",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "female",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "heterosexual",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hindu",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "homosexual_gay_or_lesbian",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "intellectual_or_learning_disability",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "jewish",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "latino",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "male",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "muslim",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "other_disability",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "other_gender",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "other_race_or_ethnicity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "other_religion",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "other_sexual_orientation",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "physical_disability",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "psychiatric_or_mental_illness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "transgender",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "white",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "created_date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "publication_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "parent_id",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "article_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "rating",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "funny",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "wow",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sad",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "likes",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "disagree",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sexual_explicit",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "identity_annotator_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "toxicity_annotator_count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "3054b210-5289-42eb-8de1-918a2907104a",
       "rows": [
        [
         "0",
         "59856",
         "0.8936170212765957",
         "haha you guys are a bunch of losers.",
         "0.0212765957446808",
         "0.0",
         "0.0212765957446808",
         "0.8723404255319149",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.25",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "2015-09-29 10:50:48.488476+00",
         "2",
         null,
         "2006",
         "rejected",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0.0",
         "4",
         "47"
        ],
        [
         "1",
         "239607",
         "0.9125",
         "Yet call out all Muslims for the acts of a few will get you pilloried.   So why is it okay to smear an entire religion over these few idiots?  Or is this because it's okay to bash Christian sects?",
         "0.05",
         "0.2375",
         "0.6125",
         "0.8875",
         "0.1125",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "2016-01-13 20:50:07.430323+00",
         "6",
         "239581.0",
         "26670",
         "approved",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0.0",
         "4",
         "80"
        ],
        [
         "2",
         "239612",
         "0.8307692307692307",
         "This bitch is nuts. Who would read a book by a woman.",
         "0.1076923076923077",
         "0.6615384615384615",
         "0.3384615384615385",
         "0.8307692307692307",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "2016-01-13 21:08:08.861688+00",
         "6",
         null,
         "26674",
         "rejected",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0.0615384615384615",
         "4",
         "65"
        ],
        [
         "3",
         "240311",
         "0.96875",
         "You're an idiot.",
         "0.03125",
         "0.0625",
         "0.0",
         "0.96875",
         "0.0",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "2016-01-15 12:34:59.545931+00",
         "111",
         null,
         "32846",
         "rejected",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "32"
        ],
        [
         "4",
         "240329",
         "0.9",
         "Who cares!? Stark trek and Star Wars fans are all dorks who never get laid.",
         "0.1",
         "0.2",
         "0.0",
         "0.9",
         "0.0",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "2015-10-13 17:16:38.081524+00",
         "111",
         null,
         "32846",
         "rejected",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0.3",
         "0",
         "10"
        ]
       ],
       "shape": {
        "columns": 45,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>239607</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>Yet call out all Muslims for the acts of a few...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.1125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26670</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>239612</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>This bitch is nuts. Who would read a book by a...</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.661538</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26674</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>4</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>240311</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>You're an idiot.</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>32846</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>240329</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>Who cares!? Stark trek and Star Wars fans are ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>32846</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    target                                       comment_text  \\\n",
       "0   59856  0.893617               haha you guys are a bunch of losers.   \n",
       "1  239607  0.912500  Yet call out all Muslims for the acts of a few...   \n",
       "2  239612  0.830769  This bitch is nuts. Who would read a book by a...   \n",
       "3  240311  0.968750                                   You're an idiot.   \n",
       "4  240329  0.900000  Who cares!? Stark trek and Star Wars fans are ...   \n",
       "\n",
       "   severe_toxicity   obscene  identity_attack    insult  threat  asian  \\\n",
       "0         0.021277  0.000000         0.021277  0.872340  0.0000    0.0   \n",
       "1         0.050000  0.237500         0.612500  0.887500  0.1125    0.0   \n",
       "2         0.107692  0.661538         0.338462  0.830769  0.0000    0.0   \n",
       "3         0.031250  0.062500         0.000000  0.968750  0.0000    NaN   \n",
       "4         0.100000  0.200000         0.000000  0.900000  0.0000    NaN   \n",
       "\n",
       "   atheist  ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
       "0      0.0  ...        2006  rejected      0    0    0      1         0   \n",
       "1      0.0  ...       26670  approved      0    0    0      1         0   \n",
       "2      0.0  ...       26674  rejected      0    0    0      0         0   \n",
       "3      NaN  ...       32846  rejected      0    0    0      0         0   \n",
       "4      NaN  ...       32846  rejected      0    0    0      0         0   \n",
       "\n",
       "   sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
       "0         0.000000                         4                        47  \n",
       "1         0.000000                         4                        80  \n",
       "2         0.061538                         4                        65  \n",
       "3         0.000000                         0                        32  \n",
       "4         0.300000                         0                        10  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Посмотрим первые 5 строк датафрейма \n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 90902 entries, 0 to 90901\n",
      "Data columns (total 45 columns):\n",
      " #   Column                               Non-Null Count  Dtype  \n",
      "---  ------                               --------------  -----  \n",
      " 0   id                                   90902 non-null  int64  \n",
      " 1   target                               90902 non-null  float64\n",
      " 2   comment_text                         90902 non-null  object \n",
      " 3   severe_toxicity                      90902 non-null  float64\n",
      " 4   obscene                              90902 non-null  float64\n",
      " 5   identity_attack                      90902 non-null  float64\n",
      " 6   insult                               90902 non-null  float64\n",
      " 7   threat                               90902 non-null  float64\n",
      " 8   asian                                21687 non-null  float64\n",
      " 9   atheist                              21687 non-null  float64\n",
      " 10  bisexual                             21687 non-null  float64\n",
      " 11  black                                21687 non-null  float64\n",
      " 12  buddhist                             21687 non-null  float64\n",
      " 13  christian                            21687 non-null  float64\n",
      " 14  female                               21687 non-null  float64\n",
      " 15  heterosexual                         21687 non-null  float64\n",
      " 16  hindu                                21687 non-null  float64\n",
      " 17  homosexual_gay_or_lesbian            21687 non-null  float64\n",
      " 18  intellectual_or_learning_disability  21687 non-null  float64\n",
      " 19  jewish                               21687 non-null  float64\n",
      " 20  latino                               21687 non-null  float64\n",
      " 21  male                                 21687 non-null  float64\n",
      " 22  muslim                               21687 non-null  float64\n",
      " 23  other_disability                     21687 non-null  float64\n",
      " 24  other_gender                         21687 non-null  float64\n",
      " 25  other_race_or_ethnicity              21687 non-null  float64\n",
      " 26  other_religion                       21687 non-null  float64\n",
      " 27  other_sexual_orientation             21687 non-null  float64\n",
      " 28  physical_disability                  21687 non-null  float64\n",
      " 29  psychiatric_or_mental_illness        21687 non-null  float64\n",
      " 30  transgender                          21687 non-null  float64\n",
      " 31  white                                21687 non-null  float64\n",
      " 32  created_date                         90902 non-null  object \n",
      " 33  publication_id                       90902 non-null  int64  \n",
      " 34  parent_id                            49830 non-null  float64\n",
      " 35  article_id                           90902 non-null  int64  \n",
      " 36  rating                               90902 non-null  object \n",
      " 37  funny                                90902 non-null  int64  \n",
      " 38  wow                                  90902 non-null  int64  \n",
      " 39  sad                                  90902 non-null  int64  \n",
      " 40  likes                                90902 non-null  int64  \n",
      " 41  disagree                             90902 non-null  int64  \n",
      " 42  sexual_explicit                      90902 non-null  float64\n",
      " 43  identity_annotator_count             90902 non-null  int64  \n",
      " 44  toxicity_annotator_count             90902 non-null  int64  \n",
      "dtypes: float64(32), int64(10), object(3)\n",
      "memory usage: 31.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на статистику датафрейма\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                         0\n",
       "target                                     0\n",
       "comment_text                               0\n",
       "severe_toxicity                            0\n",
       "obscene                                    0\n",
       "identity_attack                            0\n",
       "insult                                     0\n",
       "threat                                     0\n",
       "asian                                  69215\n",
       "atheist                                69215\n",
       "bisexual                               69215\n",
       "black                                  69215\n",
       "buddhist                               69215\n",
       "christian                              69215\n",
       "female                                 69215\n",
       "heterosexual                           69215\n",
       "hindu                                  69215\n",
       "homosexual_gay_or_lesbian              69215\n",
       "intellectual_or_learning_disability    69215\n",
       "jewish                                 69215\n",
       "latino                                 69215\n",
       "male                                   69215\n",
       "muslim                                 69215\n",
       "other_disability                       69215\n",
       "other_gender                           69215\n",
       "other_race_or_ethnicity                69215\n",
       "other_religion                         69215\n",
       "other_sexual_orientation               69215\n",
       "physical_disability                    69215\n",
       "psychiatric_or_mental_illness          69215\n",
       "transgender                            69215\n",
       "white                                  69215\n",
       "created_date                               0\n",
       "publication_id                             0\n",
       "parent_id                              41072\n",
       "article_id                                 0\n",
       "rating                                     0\n",
       "funny                                      0\n",
       "wow                                        0\n",
       "sad                                        0\n",
       "likes                                      0\n",
       "disagree                                   0\n",
       "sexual_explicit                            0\n",
       "identity_annotator_count                   0\n",
       "toxicity_annotator_count                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим на количество пропущенных значений в датафрейме\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После предварительного просмотра данных выполнил код из условия задания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "comments = df[\"comment_text\"]\n",
    "target = (df[\"target\"]>0.7).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим какие данные в наших переменных получились"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 90902 entries, 0 to 90901\n",
      "Series name: comment_text\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "90902 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 710.3+ KB\n"
     ]
    }
   ],
   "source": [
    "comments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90902,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 haha you guys are a bunch of losers.\n",
       "1    Yet call out all Muslims for the acts of a few...\n",
       "2    This bitch is nuts. Who would read a book by a...\n",
       "3                                     You're an idiot.\n",
       "4    Who cares!? Stark trek and Star Wars fans are ...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Посмотрим первые 5 строк\n",
    "display(comments.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 90902 entries, 0 to 90901\n",
      "Series name: target\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "90902 non-null  int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 710.3 KB\n"
     ]
    }
   ],
   "source": [
    "target.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90902,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: target, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Посмотрим первые 5 строк\n",
    "display(target.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "1    45451\n",
       "0    45451\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(target.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейдем к выполнению заданий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь разделим наши данные на train и test. \n",
    "\n",
    "Пусть в тест у нас пойдет 30% данных. \n",
    "\n",
    "Для этого можете использовать библиотеку train_test_split из sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ТЕСТОВАЯ ВЫБОРКА\n",
      "\n",
      "---------------------------------------\n",
      "Размер тестовой выборки: (27271,)\n",
      "---------------------------------------\n",
      "Примеры признаков:\n",
      "80470    Not meaning to belittle your practical concern...\n",
      "28773    Did you mean to imply that the bears are dying...\n",
      "76685    As a 9-year dispatch veteran, let me break it ...\n",
      "12580    A your wimp is traveling across Canada instead...\n",
      "16111    Oh Dispatch, you right wing media elitist rag....\n",
      "Name: comment_text, dtype: object\n",
      "---------------------------------------\n",
      "Примеры тестовых меток:\n",
      "80470    0\n",
      "28773    1\n",
      "76685    0\n",
      "12580    1\n",
      "16111    1\n",
      "Name: target, dtype: int64\n",
      "\n",
      "=======================================\n",
      "\n",
      "ТРЕНИРОВОЧНАЯ ВЫБОРКА\n",
      "\n",
      "---------------------------------------\n",
      "Размер тренировочной выборки: (63631,)\n",
      "---------------------------------------\n",
      "Примеры тренировочных признаков:\n",
      "12294                                    Muslim terrorist.\n",
      "57506    It's ironic that these are the same groups tha...\n",
      "56118    Star Wars has a wow factor that Star Trek does...\n",
      "28624    The settlement is 100% appropriate.\\nEnding th...\n",
      "63482    Where did it say to cover your cough with your...\n",
      "Name: comment_text, dtype: object\n",
      "---------------------------------------\n",
      "Примеры тренировочных меток:\n",
      "12294    1\n",
      "57506    0\n",
      "56118    0\n",
      "28624    1\n",
      "63482    0\n",
      "Name: target, dtype: int64\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Разделение данных на тренировочную и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    comments,  # данные\n",
    "    target,     # метки\n",
    "    test_size=0.3,  # 30% — тестовая выборка, 70% — обучающая\n",
    "    random_state=42  # фиксируем случайность для повторяемости\n",
    ")\n",
    "\n",
    "# Выводим результат\n",
    "print(\"ТЕСТОВАЯ ВЫБОРКА\\n\")\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"Размер тестовой выборки: {X_test.shape}\")\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"Примеры признаков:\\n{X_test.head()}\")\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"Примеры тестовых меток:\\n{y_test.head()}\")\n",
    "\n",
    "print(\"\\n=======================================\\n\")\n",
    "\n",
    "print(\"ТРЕНИРОВОЧНАЯ ВЫБОРКА\\n\")\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"Размер тренировочной выборки: {X_train.shape}\")\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"Примеры тренировочных признаков:\\n{X_train.head()}\")\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"Примеры тренировочных меток:\\n{y_train.head()}\")\n",
    "print(\"---------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуйте текст, который вы поделили на train и test, в числовой формат с помощью этой функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация векторизатора\n",
    "vectorizer = CountVectorizer(max_features=200)\n",
    "\n",
    "# Преобразование датафрема X_train в новый датафрейм, в котором тексты заменены на вектора  \n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "# Преобразование датафрема X_train в новый датафрейм, в котором тексты заменены на вектора  \n",
    "X_test_vec = vectorizer.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "РАЗМЕРНОСТИ ДАТАСЕТОВ\n",
      "\n",
      "---------------------------------------\n",
      "Размерность датасета X_train_vec: (63631, 200)\n",
      "Размерность датасета y_train: (63631,)\n",
      "---------------------------------------\n",
      "\n",
      "=======================================\n",
      "\n",
      "---------------------------------------\n",
      "Размерность датасета X_test_vec: (27271, 200)\n",
      "Размерность датасета y_test: (27271,)\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Сверяем размерность полученных датасетов \n",
    "\n",
    "print(\"РАЗМЕРНОСТИ ДАТАСЕТОВ\\n\")\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"Размерность датасета X_train_vec: {X_train_vec.shape}\")\n",
    "print(f\"Размерность датасета y_train: {y_train.shape}\")\n",
    "print(\"---------------------------------------\")\n",
    "\n",
    "print(\"\\n=======================================\\n\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"Размерность датасета X_test_vec: {X_test_vec.shape}\")\n",
    "print(f\"Размерность датасета y_test: {y_test.shape}\")\n",
    "print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируйте из библиотеки sklearn логистическую регрессию LogisticRegression с параметром max_iter=2000. Для оценки модели возьмите метрику accuracy и посчитайте ее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация и обучение модели логистической регрессии\n",
    "model = LogisticRegression(max_iter=2000, warm_start=True)\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Предсказание на тестовой выборке\n",
    "y_pred = model.predict(X_test_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрика Accuracy: 0.607\n"
     ]
    }
   ],
   "source": [
    "# Расчет accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Метрика Accuracy: {accuracy:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы мы смогли протестировать разные комментарии, которые приходят в голову, пропишите ниже функцию, для которой на вход мы бы подавали наш комментарий, а на выход получали предсказание, насколько от 0 до 1 комментарий является токсичным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toxicity(comment: str)-> None:\n",
    "   \"\"\"\n",
    "   Функция для предсказания токсичности комментария.\n",
    "\n",
    "   Аргументы:\n",
    "   comment (str): Комментарий, для которого нужно предсказать токсичность.\n",
    "\n",
    "   Возвращает:\n",
    "   None: Функция выводит результат предсказания на экран.\n",
    "   \"\"\"\n",
    "   # Векторизация комментария  \n",
    "   comment_vec = vectorizer.transform([comment])\n",
    "   \n",
    "   # Предсказание\n",
    "   toxicity_probability = model.predict_proba(comment_vec)[0][1]\n",
    "\n",
    "   # Вывод результата\n",
    "   if toxicity_probability > 0.5:\n",
    "      print(\"Комментарий токсичный.\")\n",
    "   else:\n",
    "      print(\"Комментарий не токсичный.\")\n",
    "\n",
    "   print(f\"Вероятность токсичности: {toxicity_probability:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте предсказать, токсичен ли комментарий «Apples are stupid». Потом предскажите, токсичен ли комментарий «I love apples»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comment_1 = \"Apples are stupid\"\n",
    "test_comment_2 = \"I love apples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Комментарий токсичный.\n",
      "Вероятность токсичности: 0.509\n"
     ]
    }
   ],
   "source": [
    "predict_toxicity(test_comment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Комментарий не токсичный.\n",
      "Вероятность токсичности: 0.480\n"
     ]
    }
   ],
   "source": [
    "predict_toxicity(test_comment_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведите десять слов, которые считаются наиболее токсичными, а также их коэффициенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 самых токсичных слов:\n",
      "---------------------------------------------\n",
      "№ | Слово | Коэффициент\n",
      "---------------------------------------------\n",
      "№1 Слово: idiot, Коэффициент: 5.732\n",
      "\n",
      "№2 Слово: idiots, Коэффициент: 5.659\n",
      "\n",
      "№3 Слово: such, Коэффициент: 5.510\n",
      "\n",
      "№4 Слово: ignorant, Коэффициент: 3.741\n",
      "\n",
      "№5 Слово: who, Коэффициент: 0.946\n",
      "\n",
      "№6 Слово: man, Коэффициент: 0.563\n",
      "\n",
      "№7 Слово: they, Коэффициент: 0.419\n",
      "\n",
      "№8 Слово: two, Коэффициент: 0.389\n",
      "\n",
      "№9 Слово: left, Коэффициент: 0.318\n",
      "\n",
      "№10 Слово: work, Коэффициент: 0.297\n",
      "\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Создаем словарь, где ключ - индекс слова, значение - само слово\n",
    "word_to_index = {word: index for index, word in vectorizer.vocabulary_.items()}\n",
    "\n",
    "# Получаем коэффициенты модели\n",
    "model_coefficients = model.coef_[0]\n",
    "\n",
    "# Получаем индексы 10 самых токсичных слов\n",
    "top_toxic_indices = np.argsort(model_coefficients)[-10:][::-1]\n",
    "\n",
    "# Получаем 10 самых токсичных слов и их коэффициенты\n",
    "top_toxic_words = [(word_to_index[i], model_coefficients[i]) for i in top_toxic_indices]\n",
    "\n",
    "# Выводим результат\n",
    "print(\"10 самых токсичных слов:\")\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"№ | Слово | Коэффициент\")\n",
    "print(\"---------------------------------------------\")\n",
    "# Ипользуем i для нумерации топа слов\n",
    "i = 1\n",
    "for word, weight in top_toxic_words:\n",
    "    print(f\"№{i} Слово: {word}, Коэффициент: {weight:.3f}\\n\")\n",
    "    i += 1\n",
    "\n",
    "\n",
    "print(\"---------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть ли слова, которых, кажется, не должно быть в списке?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проанализировав список можно заметить, что часть слов неоднозначена. Эти слова могут быть токсичным, если их использовать для оскорбления или унижения, но они также может быть использовано в нейтральном контексте. Сами по себя слова не оскорбительные и не носят только негативный характер в отличе от слов в топе таких как: idiot, idiots и ignorant, так как они часто используются для оскорбления и унижения других людей. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список слов которые под вопросов в этом списке:\n",
    "\n",
    "- Who - кто\n",
    "\n",
    "- Man - мужчина. \n",
    "\n",
    "- They - они. \n",
    "\n",
    "- Two - два. \n",
    "\n",
    "- Left - слева, осталось.\n",
    "\n",
    "- Work - работа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем протестировать модель на ее предвзятость, например, к религии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comment_list = [\n",
    "    \"I have a christian friend\",\n",
    "    \"I have a muslim friend\",\n",
    "    \"I have a white friend\",\n",
    "    \"I have a black friend\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Комментарий: I have a christian friend\n",
      "Комментарий не токсичный.\n",
      "Вероятность токсичности: 0.465\n",
      "---------------------------------------------\n",
      "Комментарий: I have a muslim friend\n",
      "Комментарий не токсичный.\n",
      "Вероятность токсичности: 0.465\n",
      "---------------------------------------------\n",
      "Комментарий: I have a white friend\n",
      "Комментарий не токсичный.\n",
      "Вероятность токсичности: 0.454\n",
      "---------------------------------------------\n",
      "Комментарий: I have a black friend\n",
      "Комментарий не токсичный.\n",
      "Вероятность токсичности: 0.465\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for comment in test_comment_list:\n",
    "    print(f\"Комментарий: {comment}\")\n",
    "    predict_toxicity(comment)\n",
    "    print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все комментарии попали в группу не токсичных. Под вопросом \"I have a black friend\", звучит не совсем политкгректно. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы заметили, что комментарии, относящиеся к исламу, с большей вероятностью будут токсичными, чем комментарии, относящиеся к другим религиям, поскольку онлайн-сообщество исламофобно. Какой тип предвзятости это может внести в вашу модель?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label bias / Measurement bias - если разметчики или автоматические системы чаще помечают нейтральные сообщения о мусульманах как токсичные из-за своих предубеждений, то модель будет повторять эту ошибку\n",
    "\n",
    "Модель будет несправедливо завышать вероятность токсичности для комментариев, связанных с исламом, даже если они нейтральныьны или позитивны, и занижать вероятность токсичности для комментариев, связанных с другими религиями, даже если они токсичны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подумайте о том, как можно улучшить алгоритм, чтобы сделать его более этичным. Напишите 1–2 идеи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Эмбеддинги не только отдельных слов, но и словосочетаний (например, биграмм или триграмм), позволяют улавливать более широкий контекст, чем анализ отдельных токенов. Увеличитчение размера окна контекста может помочь модели лучше понимать смысл фраз и выражений, что снижает вероятность ошибочной классификации.\n",
    "\n",
    "2. Семантические триплеты (например, субъект-глагол-объект) представляют собой структурированный способ извлечения смысла из текста, что может помочь в понимании намерений автора комментария. Это может быть полезно для более точной классификации комментариев за счет более глубокого анализа контекста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бонусное задание (не оценивается)\n",
    "Придумайте алгоритм, который сможет этично классифицировать комментарии на токсичные и нетоксичные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом описал в 10 задании. Можно добавить использование специальных моделей которые лучши работают с контестом такие как BERT и различные его вариции. \n",
    "Также многие коментаторы пытаются обойти фильтры, поэтому можно добавить проверку на написание токсичных коментариев латиницей или с использованием цифр и символов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
